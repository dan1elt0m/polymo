{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmark: Polymo vs Spark UDF for REST API ingestion\n",
        "\n",
        "This notebook compares the performance of Polymo's native Spark DataSource against a plain Spark UDF that fetches REST API data row-by-row. It uses the public JSONPlaceholder service so no credentials are required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Install the project extras required for the Builder / DataSource: `pip install \"polymo[builder]\"`\n",
        "- Ensure PySpark 4.x is available (Polymo's minimum requirement).\n",
        "- The JSONPlaceholder API is rate-limited; keep batch sizes modest for repeatable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "import tempfile\n",
        "import textwrap\n",
        "\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json, struct\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "from polymo import ApiReader\n",
        "\n",
        "spark = SparkSession.builder.appName(\"polymo-benchmark\").master(\"local[*]\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "spark.dataSource.register(ApiReader)\n",
        "\n",
        "config_yaml = textwrap.dedent('''\n",
        "version: 0.1\n",
        "source:\n",
        "  type: rest\n",
        "  base_url: https://jsonplaceholder.typicode.com\n",
        "stream:\n",
        "  name: posts\n",
        "  path: /posts\n",
        "  params:\n",
        "    _limit: 100\n",
        "  pagination:\n",
        "    type: none\n",
        "  infer_schema: true\n",
        "''').strip()\n",
        "\n",
        "config_dir = Path(tempfile.mkdtemp(prefix='polymo-benchmark-'))\n",
        "config_path = config_dir / 'jsonplaceholder.yml'\n",
        "config_path.write_text(config_yaml)\n",
        "config_path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def benchmark_polymo(config_path: Path) -> dict:\n",
        "    start = perf_counter()\n",
        "    df = spark.read.format('polymo').option('config_path', str(config_path)).load()\n",
        "    count = df.count()\n",
        "    elapsed = perf_counter() - start\n",
        "    return {'approach': 'Polymo DataSource', 'rows': count, 'seconds': elapsed}\n",
        "\n",
        "def benchmark_udf() -> dict:\n",
        "    ids = spark.range(1, 101).toDF('id')\n",
        "\n",
        "    def fetch_post(post_id: int) -> str:\n",
        "        url = f'https://jsonplaceholder.typicode.com/posts/{post_id}'\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "\n",
        "    spark.udf.register('fetch_post', fetch_post, StringType())\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField('userId', IntegerType()),\n",
        "        StructField('id', IntegerType()),\n",
        "        StructField('title', StringType()),\n",
        "        StructField('body', StringType()),\n",
        "    ])\n",
        "\n",
        "    start = perf_counter()\n",
        "    udf_df = ids.select(from_json(col('fetch_post(id)'), schema).alias('post'))\n",
        "    count = udf_df.count()\n",
        "    elapsed = perf_counter() - start\n",
        "    return {'approach': 'Spark UDF (requests per row)', 'rows': count, 'seconds': elapsed}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = [\n",
        "    benchmark_polymo(config_path),\n",
        "    benchmark_udf(),\n",
        "]\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "summary = pd.DataFrame(results)\n",
        "summary['throughput_rows_per_sec'] = summary['rows'] / summary['seconds']\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "The Polymo reader issues a single batched request (thanks to the configuration) and then performs the usual Spark pipeline, whereas the UDF issues one HTTP GET per row. In practice you should see Polymo outperform the row-by-row UDF both in total runtime and in rows/sec.\n",
        "\n",
        "Different APIs and pagination settings will change absolute numbers, but the pattern typically holds: pushing the REST logic down into the DataSource avoids per-row Python overhead and enables Spark to optimise the plan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}