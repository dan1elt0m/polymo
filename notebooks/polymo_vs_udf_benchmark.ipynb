{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: Polymo vs Spark UDF (Paginated API)\n",
    "\n",
    "This notebook spins up a local FastAPI service that simulates a paginated REST endpoint. Polymo batches requests while a baseline UDF calls the API once per row."
   ],
   "id": "7c3fd9df27d35d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install \"polymo[builder]\"\n",
    "```\n",
    "PySpark 4.x is required."
   ],
   "id": "19438609d08e77e9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:10:21.473070Z",
     "start_time": "2025-10-04T21:10:21.314410Z"
    }
   },
   "source": [
    "import threading\n",
    "import time\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import tempfile\n",
    "import textwrap\n",
    "\n",
    "import requests\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import uvicorn\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from polymo import ApiReader\n",
    "\n",
    "DATA = [{\"id\": i, \"value\": f\"record-{i}\"} for i in range(1, 1001)]\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "PAGE_DELAY = 0.05  # seconds\n",
    "\n",
    "def _lookup_item(item_id: int) -> dict:\n",
    "    for row in DATA:\n",
    "        if row[\"id\"] == item_id:\n",
    "            return row\n",
    "    raise HTTPException(status_code=404, detail=\"Not found\")\n",
    "\n",
    "@app.get(\"/items\")\n",
    "def list_items(_offset: int = 0, _limit: int = 200):\n",
    "    time.sleep(PAGE_DELAY)\n",
    "    start = max(_offset, 0)\n",
    "    end = min(start + _limit, len(DATA))\n",
    "    return {\n",
    "        \"data\": DATA[start:end],\n",
    "        \"meta\": {\n",
    "            \"total_records\": len(DATA),\n",
    "            \"offset\": start,\n",
    "            \"limit\": _limit,\n",
    "        },\n",
    "    }\n",
    "\n",
    "@app.get(\"/item\")\n",
    "def get_item_query(item_id: int):\n",
    "    time.sleep(PAGE_DELAY)\n",
    "    return _lookup_item(item_id)\n",
    "\n",
    "@app.get(\"/item/{item_id}\")\n",
    "def get_item(item_id: int):\n",
    "    time.sleep(PAGE_DELAY)\n",
    "    return _lookup_item(item_id)\n",
    "\n",
    "\n",
    "config = uvicorn.Config(app, host=\"127.0.0.1\", port=9876, log_level=\"warning\")\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "thread = threading.Thread(target=server.run, daemon=True)\n",
    "thread.start()\n",
    "while not server.started:\n",
    "    time.sleep(0.1)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"polymo-benchmark\").master(\"local[8]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.dataSource.register(ApiReader)"
   ],
   "id": "a72242f0e319e606",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/04 23:10:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:10:21.481088Z",
     "start_time": "2025-10-04T21:10:21.478325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_yaml = textwrap.dedent('''\n",
    "version: 0.1\n",
    "source:\n",
    "  type: rest\n",
    "  base_url: http://127.0.0.1:9876\n",
    "stream:\n",
    "  name: items\n",
    "  path: /items\n",
    "  params:\n",
    "    _limit: 200\n",
    "  pagination:\n",
    "    type: offset\n",
    "    limit_param: _limit\n",
    "    offset_param: _offset\n",
    "    page_size: 200\n",
    "    total_records_path:\n",
    "      - meta\n",
    "      - total_records\n",
    "  record_selector:\n",
    "    field_path:\n",
    "      - data\n",
    "  partition:\n",
    "    strategy: pagination\n",
    "''').strip()\n",
    "\n",
    "\n",
    "config_dir = Path(tempfile.mkdtemp(prefix=\"polymo-bench-\"))\n",
    "CONFIG_PATH = config_dir / \"mock_api.yml\"\n",
    "CONFIG_PATH.write_text(config_yaml)\n"
   ],
   "id": "4c3481b83c632a2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:10:47.960504Z",
     "start_time": "2025-10-04T21:10:21.490526Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, expr, from_json\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "import statistics\n",
    "from typing import List\n",
    "\n",
    "def benchmark_polymo(config_path: Path, *, label: str, repeats: int = 3) -> dict:\n",
    "    timings: List[float] = []\n",
    "    rows = 0\n",
    "    for _ in range(repeats):\n",
    "        start = perf_counter()\n",
    "        df = (\n",
    "            spark.read.format('polymo')\n",
    "            .option('config_path', str(config_path))\n",
    "            .load()\n",
    "        )\n",
    "        rows = df.count()\n",
    "        df.write.mode('overwrite').format('noop').save()\n",
    "        timings.append(perf_counter() - start)\n",
    "    return {'approach': label, 'rows': rows, 'seconds': statistics.median(timings)}\n",
    "\n",
    "\n",
    "def fetch_item(item_id: int) -> str:\n",
    "    url = 'http://127.0.0.1:8765/item/' + str(item_id)\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def benchmark_udf(repeats: int = 3) -> dict:\n",
    "    ids = spark.range(1, len(DATA) + 1).toDF('id')\n",
    "    spark.udf.register('fetch_item', fetch_item, StringType())\n",
    "    schema = StructType([\n",
    "        StructField('id', IntegerType()),\n",
    "        StructField('value', StringType()),\n",
    "    ])\n",
    "    timings: List[float] = []\n",
    "    rows = 0\n",
    "    for _ in range(repeats):\n",
    "        start = perf_counter()\n",
    "        udf_df = ids.select(from_json(expr('fetch_item(id)'), schema).alias('row'))\n",
    "        rows = udf_df.count()\n",
    "        udf_df.write.mode('overwrite').format('noop').save()\n",
    "        timings.append(perf_counter() - start)\n",
    "    return {'approach': 'Spark UDF (per-row requests)', 'rows': rows, 'seconds': statistics.median(timings)}\n",
    "\n",
    "\n",
    "results = [\n",
    "    benchmark_polymo(CONFIG_PATH, label='Polymo DataSource (paginated items)'),\n",
    "    benchmark_udf()\n",
    "]\n",
    "results\n"
   ],
   "id": "861e857c55f8c525",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'approach': 'Polymo DataSource (paginated items)',\n",
       "  'rows': 1000,\n",
       "  'seconds': 0.715621250012191},\n",
       " {'approach': 'Spark UDF (per-row requests)',\n",
       "  'rows': 1000,\n",
       "  'seconds': 7.729226415976882}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:11:12.987204Z",
     "start_time": "2025-10-04T21:10:47.967489Z"
    }
   },
   "source": [
    "results = [\n",
    "    benchmark_udf(),\n",
    "    benchmark_polymo(CONFIG_PATH, label='Polymo DataSource (paginated items)'),\n",
    "]\n",
    "results\n"
   ],
   "id": "70cac827cc2ab4a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/04 23:10:47 WARN SimpleFunctionRegistry: The function fetch_item replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'approach': 'Spark UDF (per-row requests)',\n",
       "  'rows': 1000,\n",
       "  'seconds': 7.62242816700018},\n",
       " {'approach': 'Polymo DataSource (paginated items)',\n",
       "  'rows': 1000,\n",
       "  'seconds': 0.7210472909791861}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:11:13.432169Z",
     "start_time": "2025-10-04T21:11:12.995110Z"
    }
   },
   "source": [
    "summary_df = spark.createDataFrame(results)\n",
    "summary_df = summary_df.withColumn(\n",
    "    \"throughput_rows_per_sec\",\n",
    "    col(\"rows\") / col(\"seconds\")\n",
    ")\n",
    "summary_df.orderBy(\"approach\").show(truncate=False)\n"
   ],
   "id": "74f317c595b279ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+----+------------------+-----------------------+\n",
      "|approach                           |rows|seconds           |throughput_rows_per_sec|\n",
      "+-----------------------------------+----+------------------+-----------------------+\n",
      "|Polymo DataSource (paginated items)|1000|0.7210472909791861|1386.8715859704496     |\n",
      "|Spark UDF (per-row requests)       |1000|7.62242816700018  |131.1917906067394      |\n",
      "+-----------------------------------+----+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "With a 50 ms delay per request and 1,000 total records, partitioning makes a big difference. Reading the collection endpoint\n",
    "with pagination batches 200 rows per call and remains the fastest approach. The plain Spark UDF performs every HTTP call serially inside Python, so it lags well behind\n",
    "both DataSource strategies. Adjust `PAGE_DELAY`, the range size, or executor parallelism to mirror your API.\n"
   ],
   "id": "e519f8baa3056643"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:11:14.436780Z",
     "start_time": "2025-10-04T21:11:13.437798Z"
    }
   },
   "source": [
    "\n",
    "server.should_exit = True\n",
    "thread.join(timeout=5)\n",
    "spark.stop()\n"
   ],
   "id": "ccadcee4cde3b42c",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
